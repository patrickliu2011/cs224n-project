{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "import numpy as np\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcotcr/work/implication-acl/env/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import qa_consistency\n",
    "import qa_consistency.dataset_utils\n",
    "import qa_consistency.implication\n",
    "import json\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: generating implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*bias_ih.*\n",
      "Did not use initialization regex that was passed: .*weight_hh.*\n",
      "Did not use initialization regex that was passed: .*bias_hh.*\n",
      "Did not use initialization regex that was passed: .*weight_ih.*\n"
     ]
    }
   ],
   "source": [
    "gen = qa_consistency.implication.ImplicationsVQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Are there 3 birds ?', 'yes', 'yeseqcount'),\n",
       " ('Are there 4 birds ?', 'no', 'n+1'),\n",
       " ('Are there any birds ?', 'yes', 'ans>0 implies some')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.implications('How many birds?', '3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This path has to have all of the VQA json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_path = '/home/marcotcr/datasets/vqa'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_v1 = qa_consistency.dataset_utils.load_vqa(vqa_path, 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating implications for all VQA v1 and v2 (question, answer) pairs. You can skip this and load my precomputed implications below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_v2 = qa_consistency.dataset_utils.load_vqav2(vqa_path, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs, all_as = qa_consistency.dataset_utils.question_answers_product(vqa_v1.questions + vqa_v2.questions, vqa_v1.all_answers + vqa_v2.all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81565 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const parse questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81565/81565 [53:54<00:00, 25.22it/s]\n",
      "  0%|          | 0/815.65 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dep parse questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered the arc_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
      "Encountered the tag_loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
      "Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n",
      "816it [23:38,  1.14it/s]                            \n"
     ]
    }
   ],
   "source": [
    "parsed_qas = gen.parse_dataset(all_qs, all_as, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "implications = [gen.implications_from_parsed(x) for x in parsed_qas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqa_v1.idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/home/marcotcr/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imps = {}\n",
    "for qa, imp in zip(parsed_qas, implications):\n",
    "    all_imps[qa.as_tuple()] = imp\n",
    "pickle.dump(all_imps, open(os.path.join(output_folder, 'vqa_imps.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here if you want to use precomputed implications (link to pkl file in the repository's README)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = '/home/marcotcr/tmp/'\n",
    "all_imps = pickle.load(open(os.path.join(output_folder, 'vqa_imps.pkl'), 'rb'))\n",
    "consistency_folder = os.path.join(output_folder, 'vqa_v1_consistency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load original predictions from your model in the official vqa format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = os.path.join(output_folder, 'orig_preds.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing:\n",
      "/home/marcotcr/tmp/vqa_v1_consistency/questions.json\n",
      "/home/marcotcr/tmp/vqa_v1_consistency/annotations.json\n"
     ]
    }
   ],
   "source": [
    "# make sure this folder exists\n",
    "qa_consistency.dataset_utils.generate_implication_vqa(vqa_v1, preds_path, all_imps, consistency_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you would have to run your model on the generated files. Let's create a fake output in the right format just for simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ids = [x['question_id'] for x in json.load(open(os.path.join(consistency_folder, 'questions.json'), 'r'))['questions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_preds_path = os.path.join(output_folder, 'consistency_preds.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump([{'question_id': q, 'answer': a} for q, a in zip(question_ids, np.random.choice(['yes', 'no'], len(question_ids)))],\n",
    "          open(fake_preds_path, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency by implication type:\n",
      "\n",
      "logeq : 50.3\n",
      "necessary_condition : 49.7\n",
      "mutex : 49.9\n",
      "\n",
      "Avg  : 50.0\n"
     ]
    }
   ],
   "source": [
    "stats = qa_consistency.dataset_utils.evaluate_consistency_vqa(consistency_folder, fake_preds_path)\n",
    "print('Consistency by implication type:')\n",
    "print()\n",
    "for x, v in stats.items():\n",
    "    if x == 'all':\n",
    "        continue\n",
    "    print('%s : %.1f' % (x, 100* v))\n",
    "print()\n",
    "print('Avg  : %.1f' % (100 * stats['all']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implication_test",
   "language": "python",
   "name": "implication_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
