{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75d45a7-8f30-4cf7-b64e-5645782ef09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import json\n",
    "import nltk \n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk  # $ pip install nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import cmudict  # >>> nltk.download('cmudict')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "\n",
    "from beliefbank_data.utils import generate_assertion, generate_question, find_constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83360dd8-9e5a-48a4-8274-0c70e3a0361d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03374e1c-dff7-49d5-847b-0a6816c6a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_path = \"beliefbank_data/constraints_v2.json\"\n",
    "facts_path = \"beliefbank_data/silver_facts.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c60643-c341-4af6-9459-c8458700b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = json.load(open(constraints_path))\n",
    "facts = json.load(open(facts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f4a25158-8d99-4088-b240-51b5b6423334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american bison', 'baboon', 'birch', 'buck', 'bull', 'calf', 'camel', 'carnivore', 'carp', 'cheetah', 'chick', 'chimpanzee', 'cock', 'crocodile', 'dog', 'dolphin', 'domestic ass', 'duck', 'earthworm', 'elephant', 'european wolf spider', 'foxhound', 'frog', 'gazelle', 'gecko', 'german shepherd', 'giant panda', 'giraffe', 'gladiolus', 'hen', 'horse', 'hound', 'howler monkey', 'hummingbird', 'jaguar', 'lamb', 'leopard', 'lion', 'livestock', 'llama', 'magpie', 'midge', 'mink', 'mullet', 'myna', 'new world blackbird', 'orchid', 'owl', 'ox', 'penguin', 'peony', 'pigeon', 'poodle', 'puppy', 'rabbit', 'rat', 'reptile', 'robin', 'rose', 'salamander', 'starling', 'tiger', 'turkey', 'whale', 'zebra']\n"
     ]
    }
   ],
   "source": [
    "# entities = list(facts.keys())\n",
    "# random.shuffle(entities)\n",
    "# dev_size = 65\n",
    "# dev_entities = sorted(entities[:dev_size])\n",
    "# eval_entities = sorted(entities[dev_size:])\n",
    "# with open(\"beliefbank_data/dev_entities.txt\", \"w\") as f:\n",
    "#     f.writelines([e + '\\n' for e in dev_entities])\n",
    "# with open(\"beliefbank_data/eval_entities.txt\", \"w\") as f:\n",
    "#     f.writelines([e + '\\n' for e in eval_entities])\n",
    "\n",
    "with open(\"beliefbank_data/dev_entities.txt\", \"r\") as f:\n",
    "    dev_entities = [e.strip() for e in f.readlines()]\n",
    "print(dev_entities)\n",
    "\n",
    "# with open(\"beliefbank_data/eval_entities.txt\", \"r\") as f:\n",
    "#     eval_entities = [e.strip() for e in f.readlines()]\n",
    "# print(eval_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31eb8693-c486-4cbb-9070-4f84b074d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of facts: 9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('american bison', 'IsA,mammal', True),\n",
       " ('american bison', 'IsA,american bison', True),\n",
       " ('american bison', 'IsA,animal', True),\n",
       " ('american bison', 'IsA,vertebrate', True),\n",
       " ('american bison', 'IsA,warm blooded animal', True)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements = [(entity, relation, label == 'yes')\n",
    "              for entity, relations in facts.items() if entity in dev_entities \n",
    "              for relation, label in relations.items()]\n",
    "print(f\"Number of facts: {len(statements)}\")\n",
    "statements[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ad53a-5ebc-4366-916e-534c78d69f44",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f2dd540c-0d05-4fec-85cc-6309b64a4764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "699dba1b-3171-4690-be58-a5fd082e80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads a pretty large model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/macaw-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/macaw-large\")\n",
    "model = model.to(device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f170c9-5046-4905-b309-e7a821357467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Model stuff\n",
    "def format_question(question_list):\n",
    "    question_list = [\"$answer$ ; $mcoptions$ = (A) yes (B) no; $question$ = \" + item \\\n",
    "         for item in question_list]\n",
    "    return question_list\n",
    "\n",
    "def predict(question_list):\n",
    "    B = len(question_list)\n",
    "    question_list = format_question(question_list)\n",
    "    answer_list_all_yes = [\"$answer$ = yes\"] * B     # pass in list of \"yes\"\n",
    "    \n",
    "    # print(dir(tokenizer))\n",
    "    inputs = tokenizer.batch_encode_plus(question_list, max_length = 256, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer.batch_encode_plus(answer_list_all_yes, max_length = 15, padding=True, truncation=True, return_tensors=\"pt\") # max_length is set to len(\"$answer$ = yes\")\n",
    "    \n",
    "    # output = model.generate(input_ids, max_length=200)\n",
    "    # answers = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    fwd = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
    "                # decoder_input_ids=labels[\"input_ids\"], decoder_attention_mask=labels[\"attention_mask\"])\n",
    "                labels=labels[\"input_ids\"])\n",
    "    # output_ids = torch.argmax(fwd.logits, dim=-1)\n",
    "    # print(tokenizer.batch_decode(output_ids, skip_special_tokens=True))\n",
    "\n",
    "    # loss\n",
    "    # loss = fwd.loss # - log(P(y|x))\n",
    "    # confidence = torch.exp(-loss)\n",
    "    logits = fwd.logits.reshape((B, 7, -1))\n",
    "    logits = logits[:, 5, :] # Index of yes/no token in answer\n",
    "    probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    # yes has input_id 4273, no has input_id 150\n",
    "    confidence_yes = probs[..., 4273] \n",
    "    confidence_no = probs[..., 150]\n",
    "    \n",
    "    answers = (confidence_yes >= confidence_no) # np.array([(ans == \"$answer$ = yes\") for ans in answers])\n",
    "    confidences = np.where(answers, confidence_yes, confidence_no)\n",
    "\n",
    "    return answers, confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59effc47-e588-4c7b-8e90-82bf14a9e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "nli_model = nli_model.to(device=device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1970157b-6933-4d87-b772-300891b1b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contradiction_matrix(sents, nli_tokenizer, nli_model):\n",
    "    if sents.ndim == 1:\n",
    "        sents = sents.reshape(1, -1)\n",
    "    \n",
    "    N, B = sents.shape\n",
    "    prem = []\n",
    "    hypo = []\n",
    "    for i in range(N):\n",
    "        for j in range(B):\n",
    "            for k in range(B):\n",
    "                prem.append(sents[i][j])\n",
    "                hypo.append(sents[i][k])\n",
    "\n",
    "    tokenized = nli_tokenizer(prem, hypo, \n",
    "                              max_length=256, \n",
    "                              return_token_type_ids=True, \n",
    "                              truncation=True,\n",
    "                              padding=True)\n",
    "    \n",
    "    input_ids = torch.Tensor(tokenized['input_ids']).to(device).long()\n",
    "    token_type_ids = torch.Tensor(tokenized['token_type_ids']).to(device).long()\n",
    "    attention_mask = torch.Tensor(tokenized['attention_mask']).to(device).long()\n",
    "    \n",
    "    nli_outputs = nli_model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=None)\n",
    "    predicted_probability = torch.softmax(nli_outputs.logits, dim=1)\n",
    "    contra_matrix = predicted_probability[..., 2]\n",
    "    contra_matrix = contra_matrix.reshape(N, B, B)\n",
    "    return contra_matrix.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e59a46-e727-479b-99ba-0475fb2a7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction methods\n",
    "def do_nothing(predictions, confidences, contra_matrix):\n",
    "    return predictions\n",
    "\n",
    "def correction_1(predictions, confidences, contra_matrix):\n",
    "    contra_matrix_sym = (contra_matrix + contra_matrix.T) / 2\n",
    "    pass\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9566f12a-18af-4d2a-a705-b56d55ca9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, answers):\n",
    "    if predictions.ndim == 1:\n",
    "        predictions = predictions.reshape(1, -1)\n",
    "    answers = answers.reshape(predictions.shape)\n",
    "    \n",
    "    actual_answers = answers.copy()\n",
    "    actual_answers[:, 1:] = np.logical_not(actual_answers[:, 1:])\n",
    "    acc = np.sum(predictions == actual_answers)\n",
    "    \n",
    "    yes_no = (answers[:, 0] == answers[:, 1])\n",
    "    pred_same = (predictions[:, 0] == predictions[:, 1])\n",
    "    pred_diff = np.logical_not(pred_same)\n",
    "    con = np.where(yes_no, pred_same, pred_diff)\n",
    "    con = np.count_nonzero(con)\n",
    "    \n",
    "    total = predictions.size\n",
    "    bsize = predictions.shape[0]\n",
    "    return acc, con, total, bsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88143abe-885c-499c-bcfc-40304aa1edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 36: 2 batches, 20 pairs\n",
      "\tAccurate 33 / 40 = 0.825\n",
      "\tContradictions 7 / 20 = 0.35\n",
      "Iter 61: 4 batches, 40 pairs\n",
      "\tAccurate 53 / 80 = 0.6625\n",
      "\tContradictions 15 / 40 = 0.375\n",
      "Iter 81: 6 batches, 60 pairs\n",
      "\tAccurate 82 / 120 = 0.6833333333333333\n",
      "\tContradictions 22 / 60 = 0.36666666666666664\n",
      "Iter 101: 8 batches, 80 pairs\n",
      "\tAccurate 110 / 160 = 0.6875\n",
      "\tContradictions 28 / 80 = 0.35\n",
      "Iter 122: 10 batches, 100 pairs\n",
      "\tAccurate 142 / 200 = 0.71\n",
      "\tContradictions 32 / 100 = 0.32\n",
      "Iter 149: 12 batches, 120 pairs\n",
      "\tAccurate 171 / 240 = 0.7125\n",
      "\tContradictions 41 / 120 = 0.3416666666666667\n",
      "Iter 183: 14 batches, 140 pairs\n",
      "\tAccurate 200 / 280 = 0.7142857142857143\n",
      "\tContradictions 50 / 140 = 0.35714285714285715\n",
      "Iter 204: 16 batches, 160 pairs\n",
      "\tAccurate 229 / 320 = 0.715625\n",
      "\tContradictions 61 / 160 = 0.38125\n",
      "Iter 224: 18 batches, 180 pairs\n",
      "\tAccurate 260 / 360 = 0.7222222222222222\n",
      "\tContradictions 70 / 180 = 0.3888888888888889\n",
      "Iter 244: 20 batches, 200 pairs\n",
      "\tAccurate 290 / 400 = 0.725\n",
      "\tContradictions 76 / 200 = 0.38\n",
      "Iter 265: 22 batches, 220 pairs\n",
      "\tAccurate 325 / 440 = 0.7386363636363636\n",
      "\tContradictions 81 / 220 = 0.36818181818181817\n",
      "Iter 293: 24 batches, 240 pairs\n",
      "\tAccurate 352 / 480 = 0.7333333333333333\n",
      "\tContradictions 92 / 240 = 0.38333333333333336\n",
      "Iter 326: 26 batches, 260 pairs\n",
      "\tAccurate 378 / 520 = 0.7269230769230769\n",
      "\tContradictions 102 / 260 = 0.3923076923076923\n",
      "Iter 347: 28 batches, 280 pairs\n",
      "\tAccurate 407 / 560 = 0.7267857142857143\n",
      "\tContradictions 103 / 280 = 0.3678571428571429\n",
      "Iter 367: 30 batches, 300 pairs\n",
      "\tAccurate 432 / 600 = 0.72\n",
      "\tContradictions 114 / 300 = 0.38\n",
      "Iter 388: 32 batches, 320 pairs\n",
      "\tAccurate 460 / 640 = 0.71875\n",
      "\tContradictions 124 / 320 = 0.3875\n",
      "Iter 415: 34 batches, 340 pairs\n",
      "\tAccurate 490 / 680 = 0.7205882352941176\n",
      "\tContradictions 130 / 340 = 0.38235294117647056\n",
      "Iter 449: 36 batches, 360 pairs\n",
      "\tAccurate 519 / 720 = 0.7208333333333333\n",
      "\tContradictions 141 / 360 = 0.39166666666666666\n",
      "Iter 470: 38 batches, 380 pairs\n",
      "\tAccurate 545 / 760 = 0.7171052631578947\n",
      "\tContradictions 149 / 380 = 0.39210526315789473\n",
      "Iter 491: 40 batches, 400 pairs\n",
      "\tAccurate 572 / 800 = 0.715\n",
      "\tContradictions 154 / 400 = 0.385\n",
      "Iter 511: 42 batches, 420 pairs\n",
      "\tAccurate 604 / 840 = 0.719047619047619\n",
      "\tContradictions 160 / 420 = 0.38095238095238093\n",
      "Iter 531: 44 batches, 440 pairs\n",
      "\tAccurate 639 / 880 = 0.7261363636363637\n",
      "\tContradictions 165 / 440 = 0.375\n",
      "Iter 558: 46 batches, 460 pairs\n",
      "\tAccurate 667 / 920 = 0.725\n",
      "\tContradictions 175 / 460 = 0.3804347826086957\n",
      "Iter 592: 48 batches, 480 pairs\n",
      "\tAccurate 697 / 960 = 0.7260416666666667\n",
      "\tContradictions 183 / 480 = 0.38125\n",
      "Iter 613: 50 batches, 500 pairs\n",
      "\tAccurate 729 / 1000 = 0.729\n",
      "\tContradictions 187 / 500 = 0.374\n",
      "Iter 633: 52 batches, 520 pairs\n",
      "\tAccurate 756 / 1040 = 0.7269230769230769\n",
      "\tContradictions 196 / 520 = 0.3769230769230769\n",
      "Iter 653: 54 batches, 540 pairs\n",
      "\tAccurate 787 / 1080 = 0.7287037037037037\n",
      "\tContradictions 203 / 540 = 0.37592592592592594\n",
      "Iter 681: 56 batches, 560 pairs\n",
      "\tAccurate 821 / 1120 = 0.7330357142857142\n",
      "\tContradictions 207 / 560 = 0.36964285714285716\n",
      "Iter 715: 58 batches, 580 pairs\n",
      "\tAccurate 854 / 1160 = 0.7362068965517241\n",
      "\tContradictions 214 / 580 = 0.3689655172413793\n",
      "Iter 736: 60 batches, 600 pairs\n",
      "\tAccurate 885 / 1200 = 0.7375\n",
      "\tContradictions 219 / 600 = 0.365\n",
      "Iter 756: 62 batches, 620 pairs\n",
      "\tAccurate 915 / 1240 = 0.7379032258064516\n",
      "\tContradictions 227 / 620 = 0.36612903225806454\n",
      "Iter 776: 64 batches, 640 pairs\n",
      "\tAccurate 946 / 1280 = 0.7390625\n",
      "\tContradictions 236 / 640 = 0.36875\n",
      "Iter 797: 66 batches, 660 pairs\n",
      "\tAccurate 979 / 1320 = 0.7416666666666667\n",
      "\tContradictions 243 / 660 = 0.36818181818181817\n",
      "Iter 832: 68 batches, 680 pairs\n",
      "\tAccurate 1011 / 1360 = 0.7433823529411765\n",
      "\tContradictions 251 / 680 = 0.36911764705882355\n",
      "Iter 853: 70 batches, 700 pairs\n",
      "\tAccurate 1035 / 1400 = 0.7392857142857143\n",
      "\tContradictions 261 / 700 = 0.37285714285714283\n",
      "Iter 873: 72 batches, 720 pairs\n",
      "\tAccurate 1058 / 1440 = 0.7347222222222223\n",
      "\tContradictions 268 / 720 = 0.37222222222222223\n",
      "Iter 893: 74 batches, 740 pairs\n",
      "\tAccurate 1082 / 1480 = 0.731081081081081\n",
      "\tContradictions 282 / 740 = 0.3810810810810811\n",
      "Iter 914: 76 batches, 760 pairs\n",
      "\tAccurate 1114 / 1520 = 0.7328947368421053\n",
      "\tContradictions 288 / 760 = 0.37894736842105264\n",
      "Iter 972: 78 batches, 780 pairs\n",
      "\tAccurate 1144 / 1560 = 0.7333333333333333\n",
      "\tContradictions 294 / 780 = 0.3769230769230769\n",
      "Iter 1002: 80 batches, 800 pairs\n",
      "\tAccurate 1176 / 1600 = 0.735\n",
      "\tContradictions 302 / 800 = 0.3775\n",
      "Iter 1027: 82 batches, 820 pairs\n",
      "\tAccurate 1204 / 1640 = 0.7341463414634146\n",
      "\tContradictions 310 / 820 = 0.3780487804878049\n",
      "Iter 1047: 84 batches, 840 pairs\n",
      "\tAccurate 1238 / 1680 = 0.736904761904762\n",
      "\tContradictions 314 / 840 = 0.3738095238095238\n",
      "Iter 1067: 86 batches, 860 pairs\n",
      "\tAccurate 1266 / 1720 = 0.736046511627907\n",
      "\tContradictions 324 / 860 = 0.3767441860465116\n",
      "Iter 1087: 88 batches, 880 pairs\n",
      "\tAccurate 1298 / 1760 = 0.7375\n",
      "\tContradictions 328 / 880 = 0.37272727272727274\n",
      "Iter 1212: 90 batches, 900 pairs\n",
      "\tAccurate 1329 / 1800 = 0.7383333333333333\n",
      "\tContradictions 335 / 900 = 0.37222222222222223\n",
      "Iter 1246: 92 batches, 920 pairs\n",
      "\tAccurate 1359 / 1840 = 0.7385869565217391\n",
      "\tContradictions 345 / 920 = 0.375\n",
      "Iter 1268: 94 batches, 940 pairs\n",
      "\tAccurate 1384 / 1880 = 0.7361702127659574\n",
      "\tContradictions 356 / 940 = 0.37872340425531914\n",
      "Iter 1288: 96 batches, 960 pairs\n",
      "\tAccurate 1417 / 1920 = 0.7380208333333333\n",
      "\tContradictions 361 / 960 = 0.37604166666666666\n",
      "Iter 1308: 98 batches, 980 pairs\n",
      "\tAccurate 1444 / 1960 = 0.736734693877551\n",
      "\tContradictions 372 / 980 = 0.3795918367346939\n",
      "Iter 1328: 100 batches, 1000 pairs\n",
      "\tAccurate 1473 / 2000 = 0.7365\n",
      "\tContradictions 381 / 1000 = 0.381\n",
      "Iter 1365: 102 batches, 1020 pairs\n",
      "\tAccurate 1506 / 2040 = 0.7382352941176471\n",
      "\tContradictions 388 / 1020 = 0.3803921568627451\n",
      "Iter 1390: 104 batches, 1040 pairs\n",
      "\tAccurate 1540 / 2080 = 0.7403846153846154\n",
      "\tContradictions 394 / 1040 = 0.37884615384615383\n",
      "Iter 1410: 106 batches, 1060 pairs\n",
      "\tAccurate 1575 / 2120 = 0.7429245283018868\n",
      "\tContradictions 399 / 1060 = 0.37641509433962267\n",
      "Iter 1430: 108 batches, 1080 pairs\n",
      "\tAccurate 1605 / 2160 = 0.7430555555555556\n",
      "\tContradictions 407 / 1080 = 0.3768518518518518\n",
      "Iter 1451: 110 batches, 1100 pairs\n",
      "\tAccurate 1636 / 2200 = 0.7436363636363637\n",
      "\tContradictions 414 / 1100 = 0.37636363636363634\n",
      "Iter 1507: 112 batches, 1120 pairs\n",
      "\tAccurate 1668 / 2240 = 0.7446428571428572\n",
      "\tContradictions 420 / 1120 = 0.375\n",
      "Iter 1532: 114 batches, 1140 pairs\n",
      "\tAccurate 1698 / 2280 = 0.7447368421052631\n",
      "\tContradictions 428 / 1140 = 0.37543859649122807\n",
      "Iter 1553: 116 batches, 1160 pairs\n",
      "\tAccurate 1725 / 2320 = 0.7435344827586207\n",
      "\tContradictions 435 / 1160 = 0.375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1219558/922791235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# print(\"Labels (for contradiction):\", answer_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mconfidences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1219558/3358997190.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(question_list)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# output = model.generate(input_ids, max_length=200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# answers = tokenizer.batch_decode(output, skip_special_tokens=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     fwd = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;31m# decoder_input_ids=labels[\"input_ids\"], decoder_attention_mask=labels[\"attention_mask\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 labels=labels[\"input_ids\"])\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1616\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1617\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 )\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1012\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/atlas/u/pliu1/anaconda3/envs/contradiction/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Batch these calculations (in particular when plugging into predict/QA model\n",
    "acc_count = 0\n",
    "con_count = 0\n",
    "total_count = 0\n",
    "num_pairs = 0\n",
    "\n",
    "batch_size = 10\n",
    "batch_counter = 0\n",
    "num_batches = 0\n",
    "batch = []\n",
    "for idx, base in enumerate(statements):\n",
    "    entity, relation, true = base\n",
    "    \n",
    "    filter_dict = {\n",
    "        'source': relation,\n",
    "        'direction': 'forward',\n",
    "    }\n",
    "    selected_constraints = find_constraints(constraints, filter_dict=filter_dict)\n",
    "    if len(selected_constraints) == 0:\n",
    "        continue\n",
    "    c = random.choice(selected_constraints)\n",
    "    contra = (entity, c['target'], not (c['weight'] == 'yes_yes'))\n",
    "    # print(base, contra)\n",
    "    \n",
    "    # batch = [base, contra]\n",
    "    if batch_counter == 0:\n",
    "        batch = []\n",
    "    batch_counter += 1\n",
    "    batch.extend([base, contra])\n",
    "    if batch_counter < batch_size: # Batch not full yet, keep accumulating examples\n",
    "        continue\n",
    "    # We have a full batch\n",
    "    batch_counter = 0\n",
    "    num_batches += 1\n",
    "    \n",
    "    questions, answers = zip(*[generate_question(*tup) for tup in batch])\n",
    "    question_list = list(questions)\n",
    "    answer_list = np.array([ans == \"Yes\" for ans in answers])\n",
    "    # print(\"Questions:\", question_list)\n",
    "    # print(\"Labels (for contradiction):\", answer_list)\n",
    "    \n",
    "    predictions, confidences = predict(question_list)\n",
    "    predictions = predictions.flatten()\n",
    "    confidences = confidences.flatten()\n",
    "    # print(\"QA predictions:\", predictions)\n",
    "    # print(\"QA confidences:\", confidences)\n",
    "    \n",
    "    pred_batch = [(ent, rel, predictions[i]) for i, (ent, rel, true) in enumerate(batch)]\n",
    "    assertions = [generate_assertion(*tup) for tup in pred_batch]\n",
    "    # print(\"Assertions:\", assertions)\n",
    "    \n",
    "    assertions = np.array(assertions).reshape(batch_size, -1)\n",
    "    contra_matrix = contradiction_matrix(assertions, nli_tokenizer, nli_model)\n",
    "    # print(\"Contradiction probability matrix:\\n\", contra_matrix)\n",
    "    \n",
    "    predictions = predictions.reshape(batch_size, -1)\n",
    "    confidences = confidences.reshape(batch_size, -1)\n",
    "    corrected = do_nothing(predictions, confidences, contra_matrix)\n",
    "    acc, con, total, bsize = evaluate(corrected, answer_list)\n",
    "    acc_count += acc\n",
    "    con_count += con\n",
    "    total_count += total\n",
    "    num_pairs += bsize\n",
    "    # print(acc, con, total, bsize)\n",
    "    \n",
    "    if num_batches % 2 == 0:\n",
    "        print(f\"Iter {idx}: {num_batches} batches, {num_pairs} pairs\")\n",
    "        print(f\"\\tAccurate {acc_count} / {total_count} = {acc_count / total_count}\")\n",
    "        print(f\"\\tContradictions {con_count} / {num_pairs} = {con_count / num_pairs}\")\n",
    "    \n",
    "print(f\"Accurate {acc_count} / {total_count} = {acc_count / total_count}\")\n",
    "print(f\"Contradictions {con_count} / {total_count // 2} = {con_count / num_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e71d0f-b54c-415e-a33f-a0180aae85b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4a174f9e1d31d9a365b2c26615a8dc2bf9473bcc2483aeb7d2d4cf28f830d3b"
  },
  "kernelspec": {
   "display_name": "contradiction",
   "language": "python",
   "name": "contradiction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
