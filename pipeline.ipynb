{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f75d45a7-8f30-4cf7-b64e-5645782ef09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import json\n",
    "import nltk \n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk  # $ pip install nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import cmudict  # >>> nltk.download('cmudict')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "\n",
    "from beliefbank_data.utils import generate_assertion, generate_question, find_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03374e1c-dff7-49d5-847b-0a6816c6a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_path = \"beliefbank_data/constraints_v2.json\"\n",
    "facts_path = \"beliefbank_data/silver_facts.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c60643-c341-4af6-9459-c8458700b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = json.load(open(constraints_path))\n",
    "facts = json.load(open(facts_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31eb8693-c486-4cbb-9070-4f84b074d8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american bison', 'IsA,mammal', True),\n",
       " ('american bison', 'IsA,american bison', True),\n",
       " ('american bison', 'IsA,animal', True),\n",
       " ('american bison', 'IsA,vertebrate', True),\n",
       " ('american bison', 'IsA,warm blooded animal', True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements = [(entity, relation, label == 'yes')\n",
    "              for entity, relations in facts.items() \n",
    "              for relation, label in relations.items()]\n",
    "statements[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "699dba1b-3171-4690-be58-a5fd082e80d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766044ae5f554408bba948b73c3c0b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Downloads a pretty large model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/macaw-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/macaw-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d5f170c9-5046-4905-b309-e7a821357467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Model stuff\n",
    "def format_question(question_list):\n",
    "    question_list = [\"$answer$ ; $mcoptions$ = (A) yes (B) no; $question$ = \" + item \\\n",
    "         for item in question_list]\n",
    "    return question_list\n",
    "\n",
    "def predict(question_list):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    B = len(question_list)\n",
    "    question_list = format_question(question_list)\n",
    "    answer_list_all_yes = [\"$answer$ = yes\"] * B     # pass in list of \"yes\"\n",
    "\n",
    "    input_ids = tokenizer.encode(question_list, max_length = 256, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = tokenizer.encode(answer_list_all_yes, max_length = 15, padding=True, truncation=True, return_tensors=\"pt\") # max_length is set to len(\"$answer$ = yes\")\n",
    "\n",
    "    # output = model.generate(input_ids, max_length=200)\n",
    "    # answers = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    fwd = model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "    # loss\n",
    "    # loss = fwd.loss # - log(P(y|x))\n",
    "    # confidence = torch.exp(-loss)\n",
    "    logits = fwd.logits.reshape((B, 7, -1))\n",
    "    logits = logits[:, 5, :] # Index of yes/no token in answer\n",
    "    probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    # yes has input_id 4273, no has input_id 150\n",
    "    confidence_yes = probs[..., 4273] \n",
    "    confidence_no = probs[..., 150]\n",
    "    \n",
    "    answers = np.array([(ans == \"$answer$ = yes\") for ans in answers])\n",
    "    confidences = np.where(answers, confidence_yes, confidence_no)\n",
    "\n",
    "    return answers, confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59effc47-e588-4c7b-8e90-82bf14a9e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nli_tokenizer = AutoTokenizer.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1970157b-6933-4d87-b772-300891b1b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contradiction_matrix(sents, nli_tokenizer, nli_model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    B = len(sents)\n",
    "    prem = []\n",
    "    hypo = []\n",
    "    for i1 in range(B):\n",
    "        for i2 in range(B):\n",
    "            prem.append(sents[i1])\n",
    "            hypo.append(sents[i2])\n",
    "\n",
    "    tokenized = nli_tokenizer(prem, hypo, \n",
    "                              max_length=256, \n",
    "                              return_token_type_ids=True, \n",
    "                              truncation=True,\n",
    "                              padding=True)\n",
    "    \n",
    "    input_ids = torch.Tensor(tokenized['input_ids']).to(device).long()\n",
    "    token_type_ids = torch.Tensor(tokenized['token_type_ids']).to(device).long()\n",
    "    attention_mask = torch.Tensor(tokenized['attention_mask']).to(device).long()\n",
    "    \n",
    "    nli_outputs = nli_model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=None)\n",
    "    predicted_probability = torch.softmax(nli_outputs.logits, dim=1)\n",
    "    contra_matrix = predicted_probability[..., 2]\n",
    "    contra_matrix = contra_matrix.reshape(B, B)\n",
    "    return contra_matrix.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8e59a46-e727-479b-99ba-0475fb2a7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction methods\n",
    "def do_nothing(predictions, confidences, contra_matrix):\n",
    "    return predictions\n",
    "\n",
    "def correction_1(predictions, confidences, contra_matrix):\n",
    "    contra_matrix_sym = (contra_matrix + contra_matrix.T) / 2\n",
    "    pass\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9566f12a-18af-4d2a-a705-b56d55ca9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, answers):\n",
    "    actual_answers = answers.copy()\n",
    "    actual_answers[1] = not actual_answers[1]\n",
    "    acc = np.count_nonzero(predictions == actual_answers)\n",
    "    if answers[0] == answers[1]:\n",
    "        con = 1 * (predictions[0] == predictions[1])\n",
    "    else:\n",
    "        con = 1 * (predictions[0] != predictions[1])\n",
    "    total = len(predictions)\n",
    "    return acc, con, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "88143abe-885c-499c-bcfc-40304aa1edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 1514, 3247, 3321, 3229, 3274, 4273,    1]])\n",
      "QA predictions: [ True]\n",
      "QA confidences: [0.9963271  0.67922425]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1960804/25002033.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QA confidences:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0massertions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_assertion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# print(\"Assertions:\", assertions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1960804/25002033.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QA confidences:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0massertions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_assertion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# print(\"Assertions:\", assertions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# TODO: Batch these calculations (in particular when plugging into predict/QA model\n",
    "\n",
    "acc_count = 0\n",
    "con_count = 0\n",
    "total_count = 0\n",
    "for idx, base in enumerate(statements):\n",
    "    entity, relation, true = base\n",
    "    \n",
    "    filter_dict = {\n",
    "        'source': relation,\n",
    "        'direction': 'forward',\n",
    "    }\n",
    "    selected_constraints = find_constraints(constraints, filter_dict=filter_dict)\n",
    "    if len(selected_constraints) == 0:\n",
    "        continue\n",
    "    c = random.choice(selected_constraints)\n",
    "    contra = (entity, c['target'], not (c['weight'] == 'yes_yes'))\n",
    "    # print(base, contra)\n",
    "    \n",
    "    batch = [base, contra]\n",
    "    \n",
    "    questions, answers = zip(*[generate_question(*tup) for tup in batch])\n",
    "    question_list = list(questions)\n",
    "    answer_list = [ans == \"Yes\" for ans in answers]\n",
    "    # print(\"Questions:\", question_list)\n",
    "    # print(\"Labels (for contradiction):\", answer_list)\n",
    "    \n",
    "    predictions, confidences = predict(question_list)\n",
    "    predictions = predictions.flatten()\n",
    "    confidences = confidences.flatten()\n",
    "    print(\"QA predictions:\", predictions)\n",
    "    print(\"QA confidences:\", confidences)\n",
    "    \n",
    "    pred_batch = [(ent, rel, predictions[i]) for i, (ent, rel, true) in enumerate(batch)]\n",
    "    assertions = [generate_assertion(*tup) for tup in pred_batch]\n",
    "    # print(\"Assertions:\", assertions)\n",
    "    \n",
    "    contra_matrix = contradiction_matrix(assertions, nli_tokenizer, nli_model)\n",
    "    # print(\"Contradiction probability matrix:\\n\", contra_matrix)\n",
    "    \n",
    "    corrected = do_nothing(predictions, confidences, contra_matrix)\n",
    "    acc, con, total = evaluate(corrected, answer_list)\n",
    "    acc_count += acc\n",
    "    con_count += con\n",
    "    total_count += count\n",
    "    # print(acc, con, count)\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Iter {idx}: {acc_count}, {con_count} / {total_count}\")\n",
    "    \n",
    "print(f\"Accurate {acc_count} / {total_count} = {acc_count / total_count}\")\n",
    "print(f\"Contradictions {con_count} / {total_count // 2} = {con_count * 2 / total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c42f29-8fc5-4634-8fbb-d81bb5984de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4a174f9e1d31d9a365b2c26615a8dc2bf9473bcc2483aeb7d2d4cf28f830d3b"
  },
  "kernelspec": {
   "display_name": "contradiction",
   "language": "python",
   "name": "contradiction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
